{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text generation with Markovify\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "This notebook is a tour of how to generate text with Markov chains! Markov chains are a simple example of *predictive text generation*, a term I use to refer to methods of text generation that make use of statistical model that, given a certain stretch of text, *predicts* which bit of text should come next, based on probabilities learned from an existing corpus of text.\n",
    "\n",
    "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text files\n",
    "\n",
    "Before we get started, we'll first need some text! Grab two [plain text files from Project Gutenberg](http://www.gutenberg.org/) (or from another source of your choice) and save them to the same directory as this notebook. (I suggest working with two files because we'll be running some code explicitly to \"compare\" two texts. Also, I think seeing two different outputs from the text generation methods discussed in this notebook will help you better understand how those methods work.) The code in the following cell loads into Python variables the contents of *two plain text files*, assigned to variables `text_a` and `text_b`. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (`\"`) intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_a = open(\"pg48722.txt\").read()\n",
    "text_b = open(\"pg1134.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are *strings*, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                     U. S. DEPARTMENT OF AGRICULTURE\n",
      "\n",
      "                       FARMERS' BULLETIN No. 1438\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        MAKING FERMENTED PICKLES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFORMATION AND DIRECTIONS for pickli\n"
     ]
    }
   ],
   "source": [
    "print(text_a[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change `text_a` to `text_b` to see the output from your second text, or change `200` to a number of your choosing.\n",
    "\n",
    "The `random.sample()` function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'f',\n",
       " 't',\n",
       " 'd',\n",
       " 'O',\n",
       " 'd',\n",
       " 'l',\n",
       " 'l',\n",
       " 'l',\n",
       " 'e',\n",
       " 'U',\n",
       " ' ',\n",
       " 'N',\n",
       " ' ',\n",
       " ' ',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'o',\n",
       " ',']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(text_b, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't incredibly helpful on its own, but you'll notice that the characters it drew (probably) more or less follow the expected letter distribution for English (i.e., lots of `e`s and `n`s and `t`s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps more interesting would be to see a randomly-sampled list of *words*. To do this, we'll make separate variables for the words in the text, using a Python function called `.split()`, which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_words = text_a.split()\n",
    "b_words = text_b.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ten random words from both text A and text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'by',\n",
       " 'combinations',\n",
       " 'which',\n",
       " 'Select',\n",
       " 'are',\n",
       " 'intervene',\n",
       " 'a',\n",
       " 'gallon',\n",
       " 'reduces']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(a_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thy',\n",
       " 'too',\n",
       " 'concealment.',\n",
       " 'is',\n",
       " 'feel',\n",
       " 'If',\n",
       " 'shame',\n",
       " 'tune',\n",
       " 'And,',\n",
       " 'thou']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(b_words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell uses Python's `Counter` object to count the *most common* letters in the first of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 8961),\n",
       " ('e', 4507),\n",
       " ('t', 2884),\n",
       " ('a', 2701),\n",
       " ('i', 2512),\n",
       " ('o', 2408),\n",
       " ('r', 2351),\n",
       " ('s', 2234),\n",
       " ('n', 2151),\n",
       " ('l', 1595),\n",
       " ('h', 1489),\n",
       " ('d', 1251)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(text_a).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the `a_words` variable gives the most frequent *words* instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 465),\n",
       " ('of', 356),\n",
       " ('a', 199),\n",
       " ('to', 191),\n",
       " ('in', 189),\n",
       " ('and', 169),\n",
       " ('is', 149),\n",
       " ('be', 135),\n",
       " ('are', 92),\n",
       " ('or', 80),\n",
       " ('for', 75),\n",
       " ('brine', 74)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the most common words in text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 636),\n",
       " ('I', 615),\n",
       " ('and', 516),\n",
       " ('to', 494),\n",
       " ('of', 423),\n",
       " ('a', 362),\n",
       " ('my', 348),\n",
       " ('you', 290),\n",
       " ('not', 247),\n",
       " ('that', 238),\n",
       " ('your', 227),\n",
       " ('have', 209)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(b_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov models\n",
    "\n",
    "I won't go into the precise details of how to implement a Markov chain text generator in this notebook. (I have written [a tutorial on this topic](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) elsewhere, however!) But I think it's helpful to understand the fundamentals of how Markov chain text generation works. The next question we’re going to try to answer is this: Given a stretch of text (say a string of characters, or run of words), what is most the most likely bit of text to come next?\n",
    "\n",
    "One way to answer this question is with an n-gram based Markov model. What's an n-gram? I'm glad you asked!\n",
    "\n",
    "### N-grams\n",
    "\n",
    "An n-gram is simply a sequence of units drawn from a longer sequence; in the case of text, the unit in question is usually a character or a word. For convenience, we'll call the unit of the n-gram is called its level; the length of the n-gram is called its order. For example, the following is a list of all unique character-level order-2 n-grams in the word condescendences:\n",
    "\n",
    "    co\n",
    "    on\n",
    "    nd\n",
    "    de\n",
    "    es\n",
    "    sc\n",
    "    ce\n",
    "    en\n",
    "    nc\n",
    "\n",
    "And the following is an excerpt from the list of all unique word-level order-5 n-grams in The Road Not Taken:\n",
    "\n",
    "    Two roads diverged in a\n",
    "    roads diverged in a yellow\n",
    "    diverged in a yellow wood,\n",
    "    in a yellow wood, And\n",
    "    a yellow wood, And sorry\n",
    "    yellow wood, And sorry I\n",
    "\n",
    "N-grams are used frequently in natural language processing and are a basic tool text analysis. Their applications range from programs that correct spelling to creative visualizations to compression algorithms to stylometrics to generative text.\n",
    "\n",
    "### What comes next?\n",
    "\n",
    "A Markov model for text begins with a list of n-grams. But in addition to making this list, we also keep track of what unit of text (word, character, etc.) *follows* each of those n-grams.\n",
    "\n",
    "Let’s do a quick example by hand. This is the same character-level order-2 n-gram analysis of the (very brief) text “condescendences” as above, but this time keeping track of all characters that follow each n-gram:\n",
    "\n",
    "| n-grams |\tnext? |\n",
    "| ------- | ----- |\n",
    "|co| n|\n",
    "|on| d|\n",
    "|nd| e, e|\n",
    "|de| s, n|\n",
    "|es| c, (end of text)|\n",
    "|sc| e|\n",
    "|ce| n, s|\n",
    "|en| d, c|\n",
    "|nc| e|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can determine that while the n-gram `co` is followed by n 100% of the time, and while the n-gram `on` is followed by `d` 100% of the time, the n-gram `de` is followed by `s` 50% of the time, and `n` the rest of the time. Likewise, the n-gram `es` is followed by `c` 50% of the time, and followed by the end of the text the other 50% of the time.\n",
    "\n",
    "Exercise: Imagine (or even better, write out) what this table might look like if you were analyzing words instead of characters, with a source text of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains: Generating text from a Markov model\n",
    "\n",
    "The Markov models we created above don't just give us interesting statistical probabilities. It also allows us generate a *new* text with those probabilities by *chaining together predictions*. Here’s how we’ll do it, starting with the order 2 character-level Markov model of `condescendences`: (1) start with the initial n-gram (`co`)—those are the first two characters of our output. (2) Now, look at the last *n* characters of output, where *n* is the order of the n-grams in our table, and find those characters in the “n-grams” column. (3) Choose randomly among the possibilities in the corresponding “next” column, and append that letter to the output. (Sometimes, as with `co`, there’s only one possibility). (4) If you chose “end of text,” then the algorithm is over. Otherwise, repeat the process starting with (2). Here’s a record of the algorithm in action:\n",
    "\n",
    "    co\n",
    "    con\n",
    "    cond\n",
    "    conde\n",
    "    conden\n",
    "    condend\n",
    "    condendes\n",
    "    condendesc\n",
    "    condendesce\n",
    "    condendesces\n",
    "    \n",
    "As you can see, we’ve come up with a word that looks like the original word, and could even be passed off as a genuine English word (if you squint at it). From a statistical standpoint, the output of our algorithm is nearly indistinguishable from the input. This kind of algorithm—moving from one state to the next, according to a list of probabilities—is known as a Markov chain generator.\n",
    "\n",
    "### Generating with Markovify\n",
    "\n",
    "Fortunately, with the invention of digital computers, you don't have to perform this algorithm by hand! In fact, Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. It comes with a lot of extra niceties that will make our lives easier, but underneath the hood, it implements an algorithm very similar to the one we just did by hand above.\n",
    "\n",
    "To install Markovify on your computer, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (0.9.4)\n",
      "Requirement already satisfied: unidecode in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from markovify) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this cell to make the library available in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `generator_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the `.make_sentence()` method to generate a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add a small quantity of spice, in a warm place and leave the hole in the head, fill the barrel for several clays, after which they are removed from the top, with washed cucumbers of as nearly as possible of uniform size.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heating too long a time to time may be required.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If corks are used sometimes.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by disabling the check altogether with `test_output=False` (note that this means the generator will occasionally return stretches of text that are present in the source text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Chief_.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the order\n",
    "\n",
    "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
    "gen_a_4 = markovify.Text(text_a, state_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "As a temperature at the distinctive flavor which are barrels this may be placed over the required 6 quarts.\n",
      "\n",
      "order 4\n",
      "The maximum is reached at or soon after the close of the active stage of fermentation.\n"
     ]
    }
   ],
   "source": [
    "print(\"order 1\")\n",
    "print(gen_a_1.make_sentence(test_output=False))\n",
    "print()\n",
    "print(\"order 4\")\n",
    "print(gen_a_4.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the level\n",
    "\n",
    "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
    "\n",
    "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_model = SentencesByChar(\"condescendences\", state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condendendencencendendes'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_char = SentencesByChar(text_a, state_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oil spices, specially the expense of the pickles are placed over again.\n"
     ]
    }
   ],
   "source": [
    "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models\n",
    "\n",
    "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)\n",
    "generator_b = markovify.Text(text_b)\n",
    "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!) \n",
    "\n",
    "Then you can create sentences using the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the barrel.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change to \"word\" for a word-level model\n",
    "level = \"char\"\n",
    "# controls the length of the n-gram\n",
    "order = 7\n",
    "# controls the number of lines to output\n",
    "output_n = 14\n",
    "# weights between the models; text A first, text B second.\n",
    "# if you want to completely exclude one model, set its corresponding value to 0\n",
    "weights = [0.7, 0.3]\n",
    "# limit sentence output to this number of characters\n",
    "length_limit = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The lines beginning with `#` are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
    "\n",
    "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEONTES, CLEOMENES and CAMILLO.\n",
      "\n",
      "The most accurs'd am I     Respecting here.\n",
      "\n",
      "SHEPHERD.\n",
      "\n",
      "_Library_        Milton Whitney.\n",
      "\n",
      "Place in the mixture in resemblance of sugar, five     Cry lost, and saw myself thus alter'd with cucumbers container are spoiled part from her life, let be.\n",
      "\n",
      "Not so held,     Saw I men scour so on the common blocks.\n",
      "\n",
      "Pickles is conducted.\n",
      "\n",
      "You might die within my pow'r     To you and to cover and being agitation with us,     False accusation     As this instrument of salt.\n",
      "\n",
      "LEONTES.\n",
      "\n",
      "Undesirable odors and cabbage by curing.\n",
      "\n",
      "You might become the point.\n",
      "\n",
      "Prithee no more; cease.\n",
      "\n",
      "No, I'll queen is well below the lactic-acid bacteria, should not prone to the     like occasionally, and anything wildly     not a paragraph.\n",
      "\n",
      "Believe me, I speak amazedly; and therein negligent; or else a footman, sweet fellow was this seems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "for i in range(output_n):\n",
    "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating with non-prose text\n",
    "\n",
    "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're *not* working with text like this. For example, let's generate some sonnets. First, download [this plaintext version of Shakespeare's sonnets](https://raw.githubusercontent.com/aparrish/plaintext-example-files/master/sonnets.txt) and keep it in the same directory as this notebook. We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the *line* is the unit now and not the *sentence*, we need to use Markovify's `NewlineText` class instead of `Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sonnets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/ky559sqj6k57cprgfnv1vcvw0000gn/T/ipykernel_77839/3631397146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msonnets_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sonnets.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msonnets_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewlineText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msonnets_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sonnets.txt'"
     ]
    }
   ],
   "source": [
    "sonnets_text = open(\"sonnets.txt\").read()\n",
    "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now make a sonnet, sorta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this with a character-level model is a bit more tricky. I've written code in the cell below that defines a new class, `LinesByCharacter` that works like `NewlineText` but operates character-by-character instead of word-by-word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesByChar(markovify.NewlineText):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a character model with the sonnets, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate new sonnets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New moods\n",
    "\n",
    "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique. First, download [this JSON file of moods](https://raw.githubusercontent.com/dariusk/corpora/9bb62927951f79bec2454f29d71b6e9b28d874b1/data/humans/moods.json) from [Corpora Project](https://github.com/dariusk/corpora/) and save to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the JSON file and grab just the list of words naming moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mood_data = json.loads(open(\"./moods.json\").read())\n",
    "moods = mood_data['moods']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to use this is to make one big string with the moods joined together with newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_text = \"\\n\".join(moods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `LinesByChar` to make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_char_model = LinesByChar(moods_text, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, new moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    print(moods_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Hayes, Brian. “Computer recreations.” Scientific American, vol. 249, no. 5, 1983, pp. 18–31. JSTOR, http://www.jstor.org/stable/24969024. (Original column from Scientific American that described how Markov chain text generation works—very readable! I can send a PDF, hit me up.)\n",
    "* [A Travesty Generator for Micros](https://elmcip.net/critical-writing/travesty-generator-micros) is a follow-up to Hayes' article that has some more theory and an actual Pascal listing (which is now mostly of only historical interest).\n",
    "* [This notebook](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) shows how to implement a Markov chain generator from scratch in Python, if you're interested in such things!\n",
    "* Lillian-Yvonne Bertram's [Travesty Generator](http://www.noemipress.org/catalog/poetry/travesty-generator/) is a striking example of Markov chains put to poetic use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
