{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db7ef0f",
   "metadata": {},
   "source": [
    "# Transformers study\n",
    "by anna lin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ae89f",
   "metadata": {},
   "source": [
    "## \"translating\" a poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4c3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c284097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (4.27.4)\n",
      "Requirement already satisfied: requests in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/annaaa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2052b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a99a1",
   "metadata": {},
   "source": [
    "For the source material, I used [Frank O'Hara's poem](https://poets.org/poem/having-coke-you) and split it up into an array of 27 sentences and randomly chose a few as the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b830a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I look', 'in the warm New York 4 o’clock light we are drifting back and forth', 'or for that matter Marino Marini when he didn’t pick the rider as carefully', 'and the fact that you move so beautifully more or less takes care of Futurism', 'between each other like a tree breathing through its spectacles', \"'Having a Coke with You\"]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# prompt = \"读万卷书行万里路\"\n",
    "prompt = \"'Having a Coke with You\\nis even more fun than going to San Sebastian, Irún, Hendaye, Biarritz, Bayonne\\nor being sick to my stomach on the Travesera de Gracia in Barcelona\\npartly because in your orange shirt you look like a better happier St. Sebastian\\npartly because of my love for you, partly because of your love for yoghurt\\npartly because of the fluorescent orange tulips around the birches\\npartly because of the secrecy our smiles take on before people and statuary\\nit is hard to believe when I’m with you that there can be anything as still\\nas solemn as unpleasantly definitive as statuary when right in front of it\\nin the warm New York 4 o’clock light we are drifting back and forth\\nbetween each other like a tree breathing through its spectacles\\n\\nand the portrait show seems to have no faces in it at all, just paint\\nyou suddenly wonder why in the world anyone ever did them\\nI look\\nat you and I would rather look at you than all the portraits in the world\\nexcept possibly for the Polish Rider occasionally and anyway it’s in the Frick\\nwhich thank heavens you haven’t gone to yet so we can go together for the first time\\nand the fact that you move so beautifully more or less takes care of Futurism\\njust as at home I never think of the Nude Descending a Staircase or\\nat a rehearsal a single drawing of Leonardo or Michelangelo that used to wow me\\nand what good does all the research of the Impressionists do them\\nwhen they never got the right person to stand near the tree when the sun sank\\nor for that matter Marino Marini when he didn’t pick the rider as carefully\\nas the horse\\nit seems they were all cheated of some marvelous experience\\nwhich is not going to go wasted on me which is why I’m telling you about it'\"\n",
    "lines = prompt.split('\\n')\n",
    "prompted = random.sample(lines, 6)\n",
    "print(prompted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fa556",
   "metadata": {},
   "source": [
    "Then I generated a new poem in two different ways.\n",
    "First, I used each line and passed it into the generator to complete it.\n",
    "(I also found out from this [thread](https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id) how to get rid of the 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.' errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f73bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is even more fun than going to san sebastian, irún, hendaye, biarritz, bayonne and la vibar.\n",
      "\n",
      "the last installment of the series is also set for premiere on december 22 at 7 p.m\n",
      "between each other like a tree breathing through its spectacles, it is often that its heart is in the centre of the tree, or in either the other side of it or in one area.\n",
      "till the body does not breathe again.\n",
      "\n",
      "in the warm new york 4 o’clock light we are drifting back and forth about the horizon. when we are about the bright blue side we are on the horizon so it is hard to discern a spot with great detail. the sun is more\n",
      "as the horse was only able to pass through the air after the game. i found myself laughing hysterically for not being able to see the horse, but the whole situation seemed to play into it and the horse was only able to pass through my arms\n",
      "or being sick to my stomach on the travesera de gracia in barcelona and my sister in mexico city in los angeles while on the road. in the last couple of weeks, i have been diagnosed with post-traumatic stress disorder, my first\n",
      "it seems they were all cheated of some marvelous experience. if it were indeed made more than the true ones, i must tell that i believe that the bible is true (that, indeed, the bible is true) in all respects, and as a\n"
     ]
    }
   ],
   "source": [
    "for p in prompted:\n",
    "    print((generator(p, pad_token_id=tokenizer.eos_token_id)[0]['generated_text']).lower(), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9215523",
   "metadata": {},
   "source": [
    "The second way I ran the poem was using the tokenizer, passing each prompt in and passing it back in 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c48763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i look back. he has never felt this. i'm in the warm new york 4 o’clock light we are drifting back and forth from new yorkers who can not remember what they came or for that matter marino marini when he didn’t pick the rider as carefully he could with other drivers in terms\n",
      "i would and the fact that you move so beautifully more or less takes care of futurism\n",
      "i'm still going through what i thought, between each other like a tree breathing through its spectacles (e-mog). in fact a simple 'having a coke with you? it will help me stay away in that area "
     ]
    }
   ],
   "source": [
    "for n in prompted:\n",
    "# for chinese prompt\n",
    "#     p = n + \" means\"\n",
    "    for i in range(10):\n",
    "    # encode the prompt\n",
    "        prompt_encoded = tokenizer([n], return_tensors=\"pt\")\n",
    "        # run a forward pass on the network\n",
    "        result = model(**prompt_encoded)\n",
    "        # get the probabilities for the next word\n",
    "        next_token_probs = result.logits[0,-1]\n",
    "        # sort by value, get the top 12 (you can change this number! try 1, or 1000)\n",
    "        nexts = torch.argsort(next_token_probs)[-12:]\n",
    "        # append the decoded ID to the current prompt\n",
    "        n += tokenizer.decode(random.choice(nexts))\n",
    "    print(n.lower(), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd763c46",
   "metadata": {},
   "source": [
    "For fun, I tried to use a Chinese proverb, and added \" stands for\" to see what it would generate to complete that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ca53405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读 stands for, the right thing you can\n",
      "万 stands for 'free palestine.' it�\n",
      "卷 stands for, in particular a small number\n",
      "书 stands for\n",
      "\\\n",
      ".\n",
      "\n",
      "i\n",
      "行 stands for, and a large group is\n",
      "万 stands for free trade, but that would\n",
      "里 stands for his brother. his name has\n",
      "路 stands for freedom from religion & labor party\n"
     ]
    }
   ],
   "source": [
    "prompted = \"读万卷书行万里路\"\n",
    "for n in prompted:\n",
    "# for chinese prompt\n",
    "    n += \" stands for\"\n",
    "    for i in range(6):\n",
    "    # encode the prompt\n",
    "        prompt_encoded = tokenizer([n], return_tensors=\"pt\")\n",
    "        # run a forward pass on the network\n",
    "        result = model(**prompt_encoded)\n",
    "        # get the probabilities for the next word\n",
    "        next_token_probs = result.logits[0,-1]\n",
    "        # sort by value, get the top 12 (you can change this number! try 1, or 1000)\n",
    "        nexts = torch.argsort(next_token_probs)[-12:]\n",
    "        # append the decoded ID to the current prompt\n",
    "        n += tokenizer.decode(random.choice(nexts))\n",
    "    print(n.lower(), end=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
